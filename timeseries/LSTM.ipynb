{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          time      us       jp       eu      cn\n",
      "0   2018-04-02  1064.4  1000.80  1311.07  169.80\n",
      "1   2018-04-03  1058.1   999.67  1301.73  168.83\n",
      "2   2018-04-04  1057.8   992.82  1298.08  168.60\n",
      "3   2018-04-05  1057.6   990.68  1299.31  168.26\n",
      "4   2018-04-06  1059.5   989.03  1297.41  168.56\n",
      "..         ...     ...      ...      ...     ...\n",
      "527 2020-05-25  1236.2  1148.08  1348.01  173.25\n",
      "528 2020-05-26  1242.0  1152.83  1353.41  173.72\n",
      "529 2020-05-27  1237.1  1150.63  1358.77  173.00\n",
      "530 2020-05-28  1233.7  1144.91  1358.92  172.15\n",
      "531 2020-05-29  1239.4  1150.63  1372.57  172.63\n",
      "\n",
      "[532 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "date=str(date.today().year)+str(date.today().month)+str(date.today().day)\n",
    "start_date=str(int(date)-100)\n",
    "key='U8U9S9MEHH8NU1X1BRBV'\n",
    "nara=['0000001','0000002','0000003','0000053']   # 나라 코드(0000001:us, 0000002:jp, 0000003:eu, 0000053:cn)\n",
    "url=[\"http://ecos.bok.or.kr/api/StatisticSearch/\"+key+\"/json/kr/1/100000/036Y001/DD/20180401/date/0000001/?/?/\",  # 환율\n",
    "    \"http://ecos.bok.or.kr/api/StatisticSearch/\"+key+\"/json/kr/1/100000/098Y001/MM/20190401/date/0101000/?/?/\",   # 기준금리\n",
    "    \"http://ecos.bok.or.kr/api/StatisticSearch/\"+key+\"/json/kr/1/100000/022Y013/MM/20190401/date/000000/?/?/\",    # 경상수지\n",
    "    ]    \n",
    "\n",
    "s_data=pd.DataFrame()\n",
    "time=[]\n",
    "us=[]\n",
    "jp=[]\n",
    "eu=[]\n",
    "cn=[]\n",
    "\n",
    "for i in range(len(nara)):\n",
    "    data=json.loads(urlopen(url[0][:-12]+nara[i]+url[0][-5:]).read())\n",
    "    data2=data['StatisticSearch']['row']\n",
    "    for k in range(len(data2)):        \n",
    "        if nara[i]==nara[0]:\n",
    "            us.append(float(data2[k]['DATA_VALUE']))\n",
    "            time.append(int(data2[k]['TIME']))\n",
    "        elif nara[i]==nara[1]:\n",
    "            jp.append(float(data2[k]['DATA_VALUE']))\n",
    "        elif nara[i]==nara[2]:\n",
    "            eu.append(float(data2[k]['DATA_VALUE']))\n",
    "        else :\n",
    "            cn.append(float(data2[k]['DATA_VALUE']))\n",
    "\n",
    "s_data['time']=time\n",
    "s_data['us']=us\n",
    "s_data['jp']=jp\n",
    "s_data['eu']=eu\n",
    "s_data['cn']=cn\n",
    "s_data['time']=pd.to_datetime(s_data.time.apply(lambda x: str(x)))\n",
    "print(s_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data=s_data[-1:]\n",
    "s_data=s_data[:-1]\n",
    "stat=pd.DataFrame()  # us\n",
    "stat2=pd.DataFrame() # jp\n",
    "stat3=pd.DataFrame() # eu\n",
    "stat4=pd.DataFrame() # cn\n",
    "\n",
    "stat['or']=st_data[['us']].reset_index()['us']\n",
    "stat2['or']=st_data[['jp']].reset_index()['jp']\n",
    "stat3['or']=st_data[['eu']].reset_index()['eu']\n",
    "stat4['or']=st_data[['cn']].reset_index()['cn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "530/530 [==============================] - 222s 420ms/step - loss: 1.3475\n",
      "Epoch 2/20\n",
      "182/530 [=========>....................] - ETA: 3:16 - loss: 0.9217"
     ]
    }
   ],
   "source": [
    "# LSTM_US\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(s_data[['us']])\n",
    "scaled_train_data = scaler.transform(s_data[['us']])\n",
    "scaled_test_data = scaler.transform(st_data[['us']])\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "n_input = 1\n",
    "n_features= 1\n",
    "generator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(200, kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu', input_shape=(n_input, n_features)))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "lstm_model.fit_generator(generator,epochs=20)\n",
    "\n",
    "lstm_predictions_scaled = list()\n",
    "\n",
    "batch = scaled_train_data[-n_input:]\n",
    "current_batch = batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(st_data)):   \n",
    "    lstm_pred = lstm_model.predict(current_batch)[0]\n",
    "    lstm_predictions_scaled.append(lstm_pred) \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)\n",
    "\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "stat['lstm']=lstm_predictions\n",
    "print(lstm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_JP\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(s_data[['jp']])\n",
    "scaled_train_data = scaler.transform(s_data[['jp']])\n",
    "scaled_test_data = scaler.transform(st_data[['jp']])\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "n_input = 1\n",
    "n_features= 1\n",
    "generator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(200, kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu', input_shape=(n_input, n_features)))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "lstm_model.fit_generator(generator,epochs=20)\n",
    "\n",
    "lstm_predictions_scaled = list()\n",
    "\n",
    "batch = scaled_train_data[-n_input:]\n",
    "current_batch = batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(st_data)):   \n",
    "    lstm_pred = lstm_model.predict(current_batch)[0]\n",
    "    lstm_predictions_scaled.append(lstm_pred) \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)\n",
    "\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "stat2['lstm']=lstm_predictions\n",
    "print(lstm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_CN\n",
    "\n",
    "random.seed(0)\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(s_data[['cn']])\n",
    "scaled_train_data = scaler.transform(s_data[['cn']])\n",
    "scaled_test_data = scaler.transform(st_data[['cn']])\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "n_input = 1\n",
    "n_features= 1\n",
    "generator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(200,  kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu', input_shape=(n_input, n_features)))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "lstm_model.fit_generator(generator,epochs=20)\n",
    "\n",
    "lstm_predictions_scaled = list()\n",
    "\n",
    "batch = scaled_train_data[-n_input:]\n",
    "current_batch = batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(st_data)):   \n",
    "    lstm_pred = lstm_model.predict(current_batch)[0]\n",
    "    lstm_predictions_scaled.append(lstm_pred) \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)\n",
    "\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "stat4['lstm']=lstm_predictions\n",
    "print(lstm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stat)\n",
    "print(stat2)\n",
    "print(stat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savgol_filter\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "smoothed_2dg = savgol_filter(s_data['us'], window_length = 5, polyorder = 2)\n",
    "smoothed_1dg = savgol_filter(s_data['us'], window_length = 5, polyorder = 1)\n",
    "smoothed_2dg=smoothed_2dg.reshape(-1,1)\n",
    "smoothed_1dg=smoothed_1dg.reshape(-1,1)\n",
    "\n",
    "# LSTM_US_savgol_filter\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rn.seed(0)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(smoothed_1dg)\n",
    "scaled_train_data = scaler.transform(smoothed_1dg)\n",
    "scaled_test_data = scaler.transform(smoothed_1dg)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "n_input = 1\n",
    "n_features= 1\n",
    "generator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(200, kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.001),activation='relu', input_shape=(n_input, n_features)))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "lstm_model.fit_generator(generator,epochs=10)\n",
    "\n",
    "lstm_predictions_scaled = list()\n",
    "\n",
    "batch = scaled_train_data[-n_input:]\n",
    "current_batch = batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(st_data)):   \n",
    "    lstm_pred = lstm_model.predict(current_batch)[0]\n",
    "    lstm_predictions_scaled.append(lstm_pred) \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)\n",
    "\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)\n",
    "print(lstm_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
